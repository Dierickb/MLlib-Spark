{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b427e556-0872-4dcc-9f0b-6739bd5011c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Procesamiento de Datos a Gran Escala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/19 22:35:49 WARN Utils: Your hostname, MacBook-Pro-de-Dierick.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.2 instead (on interface en0)\n",
      "25/11/19 22:35:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/19 22:35:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/19 22:35:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/Users/dierickbr/python_envs/general/lib/python3.13/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hpcsparkDierickBrochero</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x109d0ef90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuración local\n",
    "configuraDierickBrochero = (\n",
    "    SparkConf()\n",
    "        .set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "        .set(\"spark.executor.memory\", \"10G\")\n",
    "        .set(\"spark.executor.cores\", \"1\")\n",
    "        .set(\"spark.cores.max\", \"9\")\n",
    "        .set(\"spark.ui.port\", \"4040\")\n",
    "        .setMaster(\"local[*]\")  # <---- Spark local usando todos los cores\n",
    "        .setAppName(\"hpcsparkDierickBrochero\")\n",
    ")\n",
    "\n",
    "# Crear sesión Spark local\n",
    "sparkDierickBrochero = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .config(conf=configuraDierickBrochero)\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "sqlContext = SQLContext(sparkDierickBrochero.sparkContext)\n",
    "\n",
    "sparkDierickBrochero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3d35310-cd72-43f7-92f0-0aa308209e2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<p><strong>Objetivo: </strong> El objetivo de este cuaderno es crear un flujo sencillo de machine learning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bfa4f5b5-6254-4238-9c8d-9e911a6eda34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cargar de datos en un Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b492c75-e3c1-427c-b0dc-3b4b918727d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Este conjunto de datos consta de una etiqueta categórica con dos valores (buenos o malos), una variable categórica (color) y dos variables numéricas.\n",
    "\n",
    "Si bien los datos son sintéticos, imaginemos que este conjunto de datos representa la salud del cliente de una empresa. La columna \"color\" representa una calificación de salud categórica hecha por un representante de servicio al cliente. La columna \"laboratorio\" representa la verdadera salud del cliente. Los otros dos valores son algunas medidas numéricas de actividad dentro de una aplicación (por ejemplo, minutos de permanencia en el sitio y compras).\n",
    "\n",
    "Supongamos que queremos entrenar un modelo de clasificación en el que esperamos predecir una variable binaria, la etiqueta, a partir de los otros valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a8a4e957-88ef-457f-8946-f2e56e04cfdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 22:36:28 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /databricks-datasets/definitive-guide/data/simple-ml.\n",
      "java.io.FileNotFoundException: File /databricks-datasets/definitive-guide/data/simple-ml does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/databricks-datasets/definitive-guide/data/simple-ml. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43msparkDierickBrochero\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/databricks-datasets/definitive-guide/data/simple-ml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df.orderBy(\u001b[33m\"\u001b[39m\u001b[33mvalue2\u001b[39m\u001b[33m\"\u001b[39m).show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/general/lib/python3.13/site-packages/pyspark/sql/readwriter.py:468\u001b[39m, in \u001b[36mDataFrameReader.json\u001b[39m\u001b[34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers, useUnsafeRow)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/general/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_envs/general/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/databricks-datasets/definitive-guide/data/simple-ml. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "df = sparkDierickBrochero.read.json(\"/databricks-datasets/definitive-guide/data/simple-ml\")\n",
    "df.orderBy(\"value2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ead1a57c-db9f-43c8-9289-34599a0f709f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cfe71ef1-9c01-426e-8474-01619db854d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El conjunto de datos actual no cumple con el requisito de estar en formato de Vector y, por lo tanto, debemos transformarlo al formato adecuado.\n",
    "\n",
    "Para lograr esto en nuestro ejemplo, vamos a especificar una RFormula. Este es un lenguaje declarativo para especificar transformaciones de aprendizaje automático y es fácil de usar una vez que comprende la sintaxis.\n",
    "\n",
    "Los operadores básicos de RFormula son:\n",
    "<p>\n",
    "<p>\"~\" Destino y términos separados</p>\n",
    "<p>\"+\" Términos de Concat; \"+ 0\" significa eliminar la intersección (esto significa que la intersección y de la línea que ajustaremos será 0)</p>\n",
    "<p>\"-\" Eliminar un término; \"- 1\" significa eliminar la intersección (esto significa que la intersección y de la línea que vamos a ajustar será 0; sí, esto hace lo mismo que \"+ 0\"</p>\n",
    "<p>\":\" Interacción (multiplicación de valores numéricos o valores categóricos binarizados)</p>\n",
    "<p>\".\" Todas las columnas excepto la variable objetivo / dependiente</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf996bfa-9c00-42e6-b38c-3198c937235c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para especificar transformaciones con esta sintaxis, necesitamos importar la clase RFormula. Luego pasamos por el proceso de definir nuestra fórmula. En este caso, queremos usar todas las variables disponibles (el \".\") Y también agregar las interacciones entre valor1 y color y valor2 y color, tratándolas como características nuevas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a6a603a3-7960-464b-bfd0-efb86c6d5d31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29547c28-244f-4b87-94a3-39d56c5a8d14",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El siguiente paso es ajustar el transformador RFormula a los datos para que descubra los posibles valores de cada columna.\n",
    "\n",
    "No todos los transformadores tienen este requisito, pero debido a que RFormula manejará automáticamente las variables categóricas por nosotros, necesita determinar qué columnas son categóricas y cuáles no, así como cuáles son los valores distintos de las columnas categóricas.\n",
    "\n",
    "Por esta razón, tenemos que llamar al método fit. Una vez que llamamos a fit, devuelve una versión \"entrenada\" de nuestro transformador que luego podemos usar para transformar nuestros datos.\n",
    "\n",
    "Luego llamamos a transform en ese objeto para transformar nuestros datos de entrada en los datos de salida esperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11077b04-a475-4e39-bcf6-38836b6fc473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class='ansiout'>+-----+----+------+------------------+--------------------+-----+\n",
       "color| lab|value1|            value2|            features|label|\n",
       "+-----+----+------+------------------+--------------------+-----+\n",
       "green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
       " blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
       " blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
       "green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
       "green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
       "green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
       "  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
       "  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
       "  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
       "  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
       "  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n",
       "green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
       " blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
       " blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
       "green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
       "green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
       "green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
       "  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
       "  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
       "  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
       "+-----+----+------+------------------+--------------------+-----+\n",
       "only showing top 20 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class='ansiout'>+-----+----+------+------------------+--------------------+-----+\n|color| lab|value1|            value2|            features|label|\n+-----+----+------+------------------+--------------------+-----+\n|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n+-----+----+------+------------------+--------------------+-----+\nonly showing top 20 rows\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fittedRF = supervised.fit(df) # Ajusta\n",
    "preparedDF = fittedRF.transform(df) # Transforma\n",
    "preparedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6bc2ae8f-7b6f-4c6e-90d9-753437ebcc18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En la salida podemos ver el resultado de nuestra transformación: una columna llamada características que tiene nuestros datos sin procesar.\n",
    "\n",
    "Lo que sucede detrás de escena es bastante simple: RFormula inspecciona nuestros datos durante la llamada de ajuste y genera un objeto que transformará nuestros datos de acuerdo con la fórmula especificada.\n",
    "\n",
    "Cuando usamos este transformador, Spark convierte automáticamente nuestra variable categórica en Dobles para que podamos ingresarla en un modelo de aprendizaje automático.\n",
    "\n",
    "En particular, asigna un valor numérico a cada color posible.\n",
    "categoría, crea características adicionales para las variables de interacción entre colores y valor1 / valor2, y las coloca todas en un solo vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c11c50e-fa58-4af3-b957-c4fbd2030721",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creemos ahora un conjunto de prueba simple basado en una división aleatoria de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ecb0e02-bcd5-4957-ba7a-fa689a88d32e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a09fe42-8434-4a1a-9635-8a009c30f51c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d287b901-ac74-48c7-80ea-d62ee69c1a70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En este caso usaremos un algoritmo de clasificación llamado regresión logística.\n",
    "\n",
    "Para crear nuestro clasificador, creamos una instancia de LogisticRegression, usando la configuración predeterminada o los hiperparámetros.\n",
    "\n",
    "Luego configuramos las columnas de etiquetas y las columnas de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f829b0a-39d3-449e-97e8-bddc44904cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7793a8bf-ecf2-4070-b522-1e83c2f12b07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Antes de comenzar a entrenar este modelo, inspeccionemos los parámetros.\n",
    "\n",
    "Este método muestra una explicación de todos los parámetros para la implementación de Spark de la regresión logística.\n",
    "\n",
    "El método \"explainParams\" existe en todos los algoritmos disponibles en MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "900fa03a-7bdc-4175-8091-257a9bcdf90d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class='ansiout'>aggregationDepth: suggested depth for treeAggregate (&gt;= 2). (default: 2)\n",
       "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
       "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
       "featuresCol: features column name. (default: features, current: features)\n",
       "fitIntercept: whether to fit an intercept term. (default: True)\n",
       "labelCol: label column name. (default: label, current: label)\n",
       "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
       "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
       "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be &gt;= 0. (default: 0.0)\n",
       "maxIter: max number of iterations (&gt;= 0). (default: 100)\n",
       "predictionCol: prediction column name. (default: prediction)\n",
       "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
       "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
       "regParam: regularization parameter (&gt;= 0). (default: 0.0)\n",
       "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
       "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
       "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
       "tol: the convergence tolerance for iterative algorithms (&gt;= 0). (default: 1e-06)\n",
       "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
       "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
       "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class='ansiout'>aggregationDepth: suggested depth for treeAggregate (&gt;= 2). (default: 2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\nfeaturesCol: features column name. (default: features, current: features)\nfitIntercept: whether to fit an intercept term. (default: True)\nlabelCol: label column name. (default: label, current: label)\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\nmaxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be &gt;= 0. (default: 0.0)\nmaxIter: max number of iterations (&gt;= 0). (default: 100)\npredictionCol: prediction column name. (default: prediction)\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\nregParam: regularization parameter (&gt;= 0). (default: 0.0)\nstandardization: whether to standardize the training features before fitting the model. (default: True)\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values &gt; 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\ntol: the convergence tolerance for iterative algorithms (&gt;= 0). (default: 1e-06)\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "248c67a5-89ed-43eb-98ec-6081c7d738c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Al crear una instancia de un algoritmo no entrenado, llega el momento de ajustarlo a los datos (entrenarlo). En este caso, esto devuelve un LogisticRegressionModel.\n",
    "\n",
    "Este código iniciará un trabajo de Spark para entrenar el modelo. A diferencia de las transformaciones, el ajuste de un modelo de aprendizaje automático es ansioso y se realiza de inmediato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab7501ad-ed35-4d1a-ade0-9b9a5eb47100",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "534ade73-8c65-45e0-9ba1-7b33e2fa7009",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Una vez completado, puede usar el modelo para hacer predicciones. Lógicamente, esto significa transformar características en etiquetas.\n",
    "\n",
    "Hacemos predicciones con el método transform. Por ejemplo, podemos transformar nuestro conjunto de datos de entrenamiento para ver qué etiquetas asignó nuestro modelo a los datos de entrenamiento y cómo se comparan con los resultados reales.\n",
    "\n",
    "Realicemos esa predicción con el siguiente fragmento de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ee5de9e5-cfdd-4e9f-9470-63aa116dc1a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class='ansiout'>+-----+----------+\n",
       "label|prediction|\n",
       "+-----+----------+\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  1.0|       1.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "  0.0|       0.0|\n",
       "+-----+----------+\n",
       "only showing top 50 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class='ansiout'>+-----+----------+\n|label|prediction|\n+-----+----------+\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  1.0|       1.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n|  0.0|       0.0|\n+-----+----------+\nonly showing top 50 rows\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fittedLR.transform(train).select(\"label\", \"prediction\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "197bad2a-9e39-42b8-92ab-cd0e0dc4f4f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Nuestro siguiente paso sería evaluar manualmente este modelo y calcular métricas de rendimiento como la tasa de verdaderos positivos, la tasa de falsos negativos, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef5cc685-f7c7-41af-af98-38d74227b3aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08a020c2-f342-4837-ad49-63af3c567c61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Como probablemente haya notado, si está realizando muchas transformaciones, escribir todos los pasos y realizar un seguimiento de DataFrames termina siendo bastante tedioso.\n",
    "\n",
    "Por eso Spark incluye el concepto Pipeline.\n",
    "\n",
    "Tenga en cuenta que es esencial que las instancias de transformadores o modelos no se reutilicen en diferentes Pipeline. Cree siempre una nueva instancia de un modelo antes de crear otra Pipeline.\n",
    "\n",
    "Para asegurarnos de no sobreajustarnos, crearemos un conjunto de pruebas de holdout(un método de validación) y ajustaremos nuestros hiperparámetros en función de un conjunto de validación (tenga en cuenta que creamos este conjunto de validación basado en el conjunto de datos original, no en el preparedDF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "149a5c9b-b83e-4cf8-ae34-63bab1c68fb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec7dcb71-dd5d-4540-8066-87e83acd4c58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora que tiene un conjunto de entrenamiento y prueba, creemos las stages base en nuestra Pipeline.\n",
    "\n",
    "Una stage simplemente representa un transformador o un estimador. En nuestro caso, tendremos dos estimadores. La RFomula y el LogisticRegresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed582fda-ecc0-4be8-8301-969ffe5bef2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "02559619-d928-4166-a14a-d09cecd0d419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora, en lugar de usar manualmente nuestras transformaciones y luego ajustar nuestro modelo, simplemente las hacemos stages en la Pipeline general, como en el siguiente fragmento de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "748b2700-bcc7-4ba4-87e8-cad9d0cc542b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "212be521-a72b-4dc9-b4f5-2e634eb02a80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Entrenamiento y Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "281349c1-adcd-4ab0-b73d-bc3af76598a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora que organizó la Pipeline, el siguiente paso es el Entrenamiento.\n",
    "\n",
    "En este caso, no entrenaremos solo un modelo. Entrenaremos varias variaciones del modelo especificando diferentes combinaciones de hiperparámetros que nos gustaría que Spark probara.\n",
    "\n",
    "Luego, seleccionaremos el mejor modelo usando un evaluador que compara sus predicciones con nuestros datos de validación.\n",
    "\n",
    "Podemos probar diferentes hiperparámetros en toda la Pipeline, incluso en la fórmula de RF que usamos para manipular los datos sin procesar.\n",
    "\n",
    "En nuestro ParamGridBuilder, hay tres hiperparámetros que varían de los valores predeterminados:\n",
    "<li>Dos versiones diferentes de RFormula</li>\n",
    "<li>Tres opciones diferentes para el parámetro ElasticNet</li>\n",
    "<li>Dos opciones diferentes para el parámetro de regularización</li>\n",
    "Esto nos da un total de 12 combinaciones diferentes de estos parámetros, lo que significa que entrenaremos 12 versiones diferentes de regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e78f15e-9c00-4bdc-82c7-335affe06059",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    ".addGrid(rForm.formula, [\n",
    "\"lab ~ . + color:value1\",\n",
    "\"lab ~ . + color:value1 + color:value2\"])\\\n",
    ".addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    ".addGrid(lr.regParam, [0.1, 2.0])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcfec495-6772-478e-b31f-bece00f29fe9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora que la cuadrícula está construida, es hora de especificar nuestro proceso de evaluación. El evaluador nos permite comparar de forma automática y objetiva varios modelos con la misma métrica de evaluación.\n",
    "\n",
    "En este caso usaremos el BinaryClassificationEvaluator, que tiene una serie de métricas de evaluación potenciales.\n",
    "\n",
    "En este caso usaremos areaUnderROC, que es el área total bajo la característica operativa del receptor, una medida común de desempeño de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd5a4d04-2d70-4673-9900-2408ca6384ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    ".setMetricName(\"areaUnderROC\")\\\n",
    ".setRawPredictionCol(\"prediction\")\\\n",
    ".setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "273cd2d9-cf7d-4b15-8d40-1eb213564d94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ahora que tenemos una canalización que especifica cómo se deben transformar nuestros datos, realizaremos la selección del modelo para probar diferentes hiperparámetros en nuestro modelo de regresión logística y medir el éxito comparando su desempeño usando la métrica areaUnderROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e35b9a12-0c9e-40aa-84b1-2ce640bf5e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit()\\\n",
    ".setTrainRatio(0.75)\\\n",
    ".setEstimatorParamMaps(params)\\\n",
    ".setEstimator(pipeline)\\\n",
    ".setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bcd3e4d1-19ef-455a-8f6d-eb24ebeba5f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ejecutemos toda la Pipeline que construimos. Para revisar, la ejecución de esta canalización probará todas las versiones del modelo con el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84857ba8-1742-47a8-95ec-82909d1969f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class='ansiout'>/databricks/spark/python/pyspark/ml/util.py:838: UserWarning: Cannot find mlflow module. To enable MLflow logging, install mlflow from PyPI.\n",
       "  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class='ansiout'>/databricks/spark/python/pyspark/ml/util.py:838: UserWarning: Cannot find mlflow module. To enable MLflow logging, install mlflow from PyPI.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f78f9969-e788-40a3-9185-745075fa7cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Tambien se evalua cómo funciona el algoritmo con el conjunto de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab357264-cbf0-4287-801f-3cb3c5ada7df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class='ansiout'>0.90625</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class='ansiout'>0.90625</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01 - ML_EjemploInicial",
   "notebookOrigID": 1906006966565397,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "general (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
